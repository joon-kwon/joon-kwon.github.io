<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-10-18 sam. 23:06 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Workshop on regret, optimization &amp; games</title>
<meta name="author" content="Joon Kwon" />
<meta name="keywords" content="workshop,regret,approachability,Blackwell,optimization,games." />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="style.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Workshop on regret, optimization &amp; games
<br />
<span class="subtitle">Detailed program</span>
</h1>
<div id="outline-container-orgc184ce5" class="outline-2">
<h2 id="orgc184ce5">Wednesday, November 5th</h2>
<div class="outline-text-2" id="text-orgc184ce5">
</div>
<div id="outline-container-org6f399c7" class="outline-3">
<h3 id="org6f399c7">9:00 — Jelena Diakonikolas (University of Wisconsin&#x2013;Madison) — On Efficient Solvers for Fixed-Point Equations: Classical Results and New Twists</h3>
<div class="outline-text-3" id="text-org6f399c7">
<p>
Fixed-point operator equations—where one seeks solutions to \(T(x) =
x\) for an operator \(T\) mapping a vector space to itself—form a
fundamental class of problems with broad applications in optimization
theory, game theory, economics, and, more recently, reinforcement
learning. In this talk, I will begin by reviewing classical
algorithmic results for solving such equations under oracle access to
the operator \(T\). I will then highlight key gaps in the literature,
particularly in settings where \(T\) may be expansive—though in a
controlled sense. Finally, I will present recent results that address
some of these challenges and conclude with open questions and
potential directions for future work.
</p>
</div>
</div>
<div id="outline-container-org7ad1d8d" class="outline-3">
<h3 id="org7ad1d8d">9:45 — Nicolò Cesa-Bianchi (Università degli Studi di Milano &amp; Politecnico di Milano) — Regret, Games and Boosting: a Reverie of Gambles and Bounds</h3>
<div class="outline-text-3" id="text-org7ad1d8d">
<p>
We revisit the celebrated boosting theorem through the lenses of game
theory and online learning. This perspective enables a principled
extension of boosting theory to more general notions of losses,
including multi-objective criteria. Our analysis characterizes the
frontier between boostable guarantees and random guessing.
</p>

<p>
Joint work with: Marco Bressan, Nataly Brukhim, Emmanuel Esposito, Yishay Mansour, Shay Moran, Max Thiessen.
</p>
</div>
</div>
<div id="outline-container-orgf23e9e3" class="outline-3">
<h3 id="orgf23e9e3">11:00 — Yishay Mansour (Tel Aviv University) — Swap Regret and Correlated Equilibria Beyond Normal-Form Games</h3>
<div class="outline-text-3" id="text-orgf23e9e3">
<p>
Swap regret is a notion that has proven itself to be central to the
study of general-sum normal-form games, with swap-regret minimization
leading to convergence to the set of correlated equilibria and
guaranteeing non-manipulability against a self-interested opponent.
However, the situation for more general classes of games &#x2013; such as
Bayesian games and extensive-form games &#x2013; is less clear-cut, with
multiple candidate definitions for swap-regret but no known
efficiently minimizable variant of swap regret that implies analogous
non-manipulability guarantees.
</p>

<p>
In this paper, we present a new variant of swap regret for polytope
games that we call "profile swap regret", with the property that
obtaining sublinear profile swap regret is both necessary and
sufficient for any learning algorithm to be non-manipulable by an
opponent (resolving an open problem of Mansour et al., 2022). Although
we show profile swap regret is NP-hard to compute given a transcript
of play, we show it is nonetheless possible to design efficient
learning algorithms that guarantee at most \(\sqrt{T})\) profile swap
regret. Finally, we explore the correlated equilibrium notion induced
by low-profile-swap-regret play, and demonstrate a gap between the set
of outcomes that can be implemented by this learning process and the
set of outcomes that can be implemented by a third-party mediator (in
contrast to the situation in normal-form games).
</p>

<p>
Joint work with Eshwar Ram Arunachaleswaran, Natalie Collina, Mehryar
Mohri, Jon Schneider, and Balasubramanian Sivan
</p>
</div>
</div>
<div id="outline-container-org2d00293" class="outline-3">
<h3 id="org2d00293">13:30 — Sergiu Hart (The Hebrew University of Jerusalem) — "Calibeating": Beating Forecasters at Their Own Game</h3>
<div class="outline-text-3" id="text-org2d00293">
<p>
In order to identify expertise, forecasters should not be tested by their
calibration score, which can always be made arbitrarily small, but rather
by their Brier score. The Brier score is the sum of the calibration score
and the refinement score; the latter measures how good the sorting into
bins with the same forecast is, and thus attests to "expertise". This
raises the question of whether one can gain calibration without losing
expertise, which we refer to as "calibeating". We provide an easy way to
calibeat any forecast, by a deterministic online procedure. We moreover
show that calibeating can be achieved by a stochastic procedure that is
itself calibrated, and then extend the results to simultaneously
calibeating multiple procedures, and to deterministic procedures that are
continuously calibrated. Finally, we show that calibeating is stronger
than the "best expert".
</p>

<p>
Joint work with Dean Foster
</p>
</div>
</div>
<div id="outline-container-org18b1087" class="outline-3">
<h3 id="org18b1087">14:15 — Gabriele Farina (MIT) — What is the strongest notion of regret that can be kept meaningfully sublinear in convex games?</h3>
<div class="outline-text-3" id="text-org18b1087">
<p>
Φ-equilibria &#x2013; and the associated notion of Φ-regret &#x2013; are a
powerful and flexible framework at the heart of online learning and
game theory, whereby enriching the set of deviations Φ begets stronger
notions of rationality. We discuss several recent positive results in
the direction of characterizing the largest notion of rationality that
can be guaranteed efficiently in general convex games. Specifically,
we show that linear and low-degree-polynomial correlated
equilibria—relaxations of correlated equilibria and a strengthening of
coarse correlated equilibria—can be computed and learned efficiently
in general convex games, including extensive-form games. Our results
are enabled by a generalization of the seminal framework of Gordon et
al. for Φ-regret minimization, providing extensions to this framework
that can be used even when the set of deviations Φ is intractable to
separate/optimize over. We sidestep the lack of tractability of Φ by
means of a new algorithmic routine that we coin semiseparation. We
discuss further applications of this algorithmic primitive in the
direction of equilibrium computation towards the end of the talk.
</p>

<p>
Based on joint work that has appeared at STOC’25 and ACM EC’25.
</p>
</div>
</div>
<div id="outline-container-orgb1c9bff" class="outline-3">
<h3 id="orgb1c9bff">15:30 — Evgenii Chzhen (CNRS) — About an extension of Blackwell’s approachability framework with applications to fairness</h3>
<div class="outline-text-3" id="text-orgb1c9bff">
<p>
We provide a setting and a general approach to fair online learning with stochastic sensitive and non-sensitive contexts. The setting is a repeated game between
the Player and Nature, where at each stage both pick actions based on the contexts. Inspired by the notion of unawareness, we assume that the Player can only
access the non-sensitive context before making a decision, while we discuss both
cases of Nature accessing the sensitive contexts and Nature unaware of the sensitive contexts. Adapting Blackwell’s approachability theory to handle the case of
an unknown contexts' distribution, we provide a general necessary and sufficient
condition for learning objectives to be compatible with some fairness constraints.
This condition is instantiated on (group-wise) no-regret and (group-wise) calibration objectives, and on demographic parity as an additional constraint. When the
objective is not compatible with the constraint, the provided framework permits to
characterise the optimal trade-off between the two.
</p>
</div>
</div>
<div id="outline-container-org982cda1" class="outline-3">
<h3 id="org982cda1">16:15 — Kohei Hatano (Kyushu University) — TBA</h3>
</div>
</div>
<div id="outline-container-org2bce809" class="outline-2">
<h2 id="org2bce809">Thursday, November 6th</h2>
<div class="outline-text-2" id="text-org2bce809">
</div>
<div id="outline-container-org406dd06" class="outline-3">
<h3 id="org406dd06">9:00 — Flore Sentenac (HEC) — Balancing optimism and pessimism in offline-to-online learning</h3>
<div class="outline-text-3" id="text-org406dd06">
<p>
Based on a paper with Ilbin Lee and Csaba Szeppesvari. 
</p>

<p>
We consider what we call the offline-to-online learning setting,
focusing on stochastic finite-armed bandit problems. In
offline-to-online learning problems, a learner starts with offline
data collected from interaction with an unknown environment in a way
that is not under the control of the learner. Given this data, the
learner then starts interacting with the environment, gradually
improving its initial strategy as it collects more data so as to
maximize the total reward it receives from the environment. The basic
dilemma of the learner in this setting is as follows: if the problem
was purely offline (the learner is given data that it can use to
design a policy, which then remains fixed), the best strategy (in a
number of senses) would be to use the Lower Confidence Bound (LCB)
algorithm, which is based on pessimism. Among other things, LCB can
simultaneously compete with any policy that is sufficiently
"covered" by the offline data. However, if the problem was purely
online, the best strategy (in a number of senses) would be to use
theUpper Confidence Bound (UCB) algorithm based on optimism. Indeed,
as time goes by, the UCB algorithm will match the performance of the
optimal policy at a speed that is nearly the best possible among all
online algorithms. For offline-to-online learning, however, UCB will
explore too much initially, and as such, its performance will be
inferior to that of LCB, at least for some time. This suggests that
the learner should use LCB initially and then gradually change its
strategy to resemble UCB. Just how this should be done (and why) is
the subject of this talk. In particular, our main result shows that
our new algorithm performs nearly as well as the better of LCB and UCB
at any point in time.
</p>
</div>
</div>
<div id="outline-container-orgb373da5" class="outline-3">
<h3 id="orgb373da5">9:45 — Elad Hazan (Princeton University) — Learning in Dynamical Systems</h3>
<div class="outline-text-3" id="text-orgb373da5">
<p>
Learning in dynamical systems is a fundamental challenge underlying
modern sequence modeling. Despite extensive study, efficient
algorithms with formal guarantees for general nonlinear systems have
remained elusive. This talk presents a provably efficient framework
for learning in any bounded and Lipschitz nonlinear dynamical system,
establishing the first sublinear regret guarantees in a dimension-free
setting. Our approach combines Koopman lifting, Luenberger observers,
and, crucially, spectral filtering to show that nonlinear dynamics are
learnable. These insights motivate a new neural architecture, the
Spectral Transform Unit (STU), which achieves state-of-the-art
performance on language modeling and dynamical system benchmarks.
</p>
</div>
</div>
<div id="outline-container-org274a2ad" class="outline-3">
<h3 id="org274a2ad">11:00 — Jacob Abernethy (Georgia Institute of Technology) — TBA</h3>
</div>
<div id="outline-container-orgd75df46" class="outline-3">
<h3 id="orgd75df46">13:30 — Francesco Orabona (KAUST) — New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results</h3>
<div class="outline-text-3" id="text-orgd75df46">
<p>
The Polyak stepsize has been proven to be a fundamental stepsize in
convex optimization, giving near optimal gradient descent rates across
a wide range of assumptions. The universality of the Polyak stepsize
has also inspired many stochastic variants, with theoretical
guarantees and strong empirical performance. Despite the many
theoretical results, our understanding of the convergence properties
and shortcomings of the Polyak stepsize or its variants is both
incomplete and fractured across different analyses. We propose a new,
unified, and simple perspective for the Polyak stepsize and its
variants as gradient descent on a surrogate loss. We show that each
variant is equivalent to minimize a surrogate function with stepsizes
that adapt to a guaranteed local curvature. Our general surrogate loss
perspective is then used to provide a unified analysis of existing
variants across different assumptions. Moreover, we show a number of
negative results proving that the non-convergence results in some of
the upper bounds is indeed real. This is a joint work with Ryan
D'Orazio.
</p>
</div>
</div>
<div id="outline-container-org5023b80" class="outline-3">
<h3 id="org5023b80">14:15 — Kfir Levy (Technion) — Do Stochastic, Feel Noiseless: Stable Optimization via a Double Momentum Mechanism</h3>
<div class="outline-text-3" id="text-org5023b80">
<p>
The tremendous success of the Machine Learning paradigm heavily relies
on the development of powerful optimization methods, and the canonical
algorithm for training learning models is SGD (Stochastic Gradient
Descent). Nevertheless, the latter is quite different from Gradient
Descent (GD) which is its noiseless counterpart. Concretely, SGD
requires a careful choice of the learning rate, which relies on the
properties of the noise as well as the quality of initialization. It
further requires the use of a test set to estimate the generalization
error throughout its run. In this talk, we will present a new SGD
variant that obtains the same optimal rates as SGD, while using
noiseless machinery as in GD. Concretely, it enables to use the same
fixed learning rate as GD and does not require to employ a
test/validation set. Curiously, our results rely on a novel gradient
estimate that combines two recent mechanisms which are related to the
notion of momentum. Finally, I will discuss several applications of
our new variant.
</p>
</div>
</div>
<div id="outline-container-org1492aff" class="outline-3">
<h3 id="org1492aff">15:30 — Ashok Cutkosky (Boston University) — How to Use Regret Bounds?</h3>
<div class="outline-text-3" id="text-org1492aff">
<p>
Modern online optimization algorithms feature significantly more
refined theoretical guarantees than were available a decade ago. By
contrast, the ways in which we attempt to apply these algorithms have
advanced much more slowly, and with much less success. In this talk, I
will discuss challenges that arise in practical application of
theoretically-motivated optimization methods to modern neural
networks, and some steps towards principled ways to convert regret
bounds into convergence guarantees. From a technical perspective, we
will see how to apply online (convex) optimization algorithms to
non-convex optimization problems, and in the process recover and
sometimes improve many prior non-convex optimization guarantees with
an arguably simpler proof. I will also discuss some empirical results
that describe places where common analytical models both succeed and
fail to capture optimization behavior, and how these results motivate
new open questions in optimization theory.
</p>
</div>
</div>
<div id="outline-container-org72dee8d" class="outline-3">
<h3 id="org72dee8d">16:15 — Aaron Defazio (Meta) — TBA</h3>
</div>
</div>
<div id="outline-container-org355609d" class="outline-2">
<h2 id="org355609d">Friday, November 7th</h2>
<div class="outline-text-2" id="text-org355609d">
</div>
<div id="outline-container-org4e8f225" class="outline-3">
<h3 id="org4e8f225">9:00 — Tomer Koren (Tel Aviv University) — Beyond Online-to-Batch: Exploring the Generalization Ability of Convex SGD</h3>
<div class="outline-text-3" id="text-org4e8f225">
<p>
A classic and fundamental result in convex optimization, since Nemirovski and Yudin (1983), is that one-pass stochastic gradient descent (SGD) converges optimally in terms of population excess risk for any convex Lipschitz objective, independently of the problem dimension. Modernly, this is often viewed as a direct consequence of regret minimization through an online-to-batch conversion. In this talk, I will discuss several basic questions concerning this seminal result:
</p>
<ol class="org-ol">
<li>What form of capacity control enables this optimal out-of-sample performance of SGD?</li>
<li>At what rate does SGD minimize the empirical risk?</li>
<li>What happens beyond the first pass? How quickly does the population performance deteriorate (if at all)?</li>
</ol>
<p>
I will present several results that shed light on these questions and reveal intriguing phenomena in the generalization behavior of SGD within the classical convex setting.
</p>

<p>
Based on joint work with Matan Schliserman, Uri Sherman, Shira Vansover-Hager, and Roi Livni.
</p>
</div>
</div>
<div id="outline-container-org45a50a2" class="outline-3">
<h3 id="org45a50a2">9:45 — Tim van Erven (University of Amsterdam) — An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction</h3>
<div class="outline-text-3" id="text-org45a50a2">
<p>
I will present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces
this setting to misspecification-robust adversarial linear bandits
with fixed action sets. Without knowledge of the context distribution
or access to a context simulator, the algorithm achieves
\(\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log K}\})\) regret and runs in
\(\text{poly}(d,C,T)\) time, where d is the feature dimension, C is an
upper bound on the number of linear constraints defining the action
set in each round, \(K\) is an upper bound on the number of actions in
each round, and \(T\) is number of rounds. This resolves the open question
by Liu et al. (2023) on whether one can obtain \(\text{poly}(d)\sqrt{T}\)
regret in polynomial time independent of the number of actions. For
the important class of combinatorial bandits with adversarial losses
and stochastic action sets where the action sets can be described by a
polynomial number of linear constraints, our algorithm is the first to
achieve \(\text{poly}(d)\sqrt{T}\) regret in polynomial time, while no
prior algorithm achieves even \(o(T)\) regret in polynomial time to our
knowledge. When a simulator is available, the regret bound can be
improved to \(\tilde{O}(d\sqrt{L^\star})\), where \(L^\star\) is the
cumulative loss of the best policy.
</p>

<p>
This is joint work with Jack Mayo, Julia Olkhovskaya and Chen-Yu Wei. 
</p>
</div>
</div>
<div id="outline-container-orgadfe953" class="outline-3">
<h3 id="orgadfe953">11:00 — Emilie Kaufmann (CNRS) — Multi-objective bandits revisited</h3>
<div class="outline-text-3" id="text-orgadfe953">
<p>
In multi-objective bandit models, an agent sequentially sample arms and collect multi-variate observations. The arm’s expected payoff are thus vectors in Rd and there is no obvious notion of “best arm” as some arms may be better for some objective but worse for others. In this talk, I will present algorithms for the adaptive identification of the Pareto set of the arms means, in a fixed-confidence setting. These algorithms adaptively sample the arms and also adaptively stop the data collection process so as to guarantee an error at most δ for their guess of the Pareto set. I will present a first algorithm based on confidence regions that achieves a near-optimal sample complexity bound featuring some appropriate notions of “sub-optimality gap”. Then we will discuss asymptotically optimal algorithm, i.e. algorithms whose sample complexity is matching a lower bound in the regime in which the error probability is small.
</p>

<p>
This talk is based on joint works with Cyrille Koné, Laura Richert and
Marc Jourdan — <a href="https://arxiv.org/abs/2307.00424">https://arxiv.org/abs/2307.00424</a> —
<a href="https://arxiv.org/abs/2411.04939">https://arxiv.org/abs/2411.04939</a>
</p>
</div>
</div>
<div id="outline-container-org7ab7c4d" class="outline-3">
<h3 id="org7ab7c4d">13:30 — Michael I. Jordan (University of California, Berkeley &amp; INRIA) — Sequential Hypothesis Tests and Universal Log-Optimality for General Classes of E-processes</h3>
<div class="outline-text-3" id="text-org7ab7c4d">
<p>
We consider the problem of sequential hypothesis testing by betting.
For a general class of composite testing problems&#x2014;which include
bounded mean testing, equal mean testing for bounded random tuples,
and some key ingredients of two-sample and independence testing as
special cases&#x2014;we show that any e-process satisfying a certain
sublinear regret bound is adaptively, asymptotically, and almost
surely log-optimal for a composite alternative. This is a strong
notion of optimality that has not previously been established for the
aforementioned problems and we provide explicit test supermartingales
and e-processes satisfying this notion in the more general case.
Furthermore, we derive matching lower and upper bounds on the expected
rejection time for the resulting sequential tests in all of these
cases. The proofs of these results make weak, algorithm-agnostic
moment assumptions and rely on a general-purpose proof technique
involving the aforementioned regret and a family of numeraire
portfolios.
</p>

<p>
Joint work with Ian Waudby-Smith and Ricardo Sandoval
</p>
</div>
</div>
<div id="outline-container-orgaa56dee" class="outline-3">
<h3 id="orgaa56dee">14:15 — Sarah Sachs (University of Bristol) — Tracking Solutions of Time-Varying Variational Inequalities</h3>
<div class="outline-text-3" id="text-orgaa56dee">
<p>
We can compute the solution of a fixed variational inequality under
sufficient assumptions. But what happens if the variational inequality
varies over time? Can we track the solutions in an online setting?
</p>

<p>
In this talk, I will present our recent results on tracking guarantees
for solutions of time-varying variational inequalities [1]. Since
variational inequality problems generalize equilibrium computation in
concave games, minimization of convex functions, and parameter
estimation in generalized linear models, our results have applications
in game theory, dynamic regret minimization, and parameter estimation
in statistics. Existing prior work has primarily focused on
time-varying games or time-varying optimization problems. For strongly
convex optimization problems or strongly monotone games, such results
provide tracking guarantees under the assumption that the variation of
the time-varying problem is restrained, that is, problems with a
sublinear solution path. We extend existing results in two ways: In
our first result, we provide tracking bounds for (1) variational
inequalities with a sublinear solution path but not necessarily
monotone functions, and (2) for periodic time-varying variational
inequalities that do not necessarily have a sublinear solution path
length. Our second main contribution is an extensive study of the
convergence behavior and trajectory of discrete dynamical systems of
periodic time-varying variational inequalities. We show that these
systems can either exhibit provably chaotic behavior or can converge
to the solution. I will conclude this talk by highlighting open
research questions and discussing potential future directions in this
area.
</p>

<p>
[1] Tracking solutions of time-varying variational inequalities. Joint work with Hédi Hadiji and Cristobál Guzmán.
</p>
</div>
</div>
<div id="outline-container-org5bab51b" class="outline-3">
<h3 id="org5bab51b">15:30 — Francis Bach (INRIA) — TBA</h3>
</div>
<div id="outline-container-org3355faf" class="outline-3">
<h3 id="org3355faf">16:15 — Roi Livni (Tel Aviv University) — Online Proper Learning with Partial Feedback and Non-Random Counterexamples</h3>
<div class="outline-text-3" id="text-org3355faf">
<p>
Online Proper Learning with Partial Feedback and Non-Random Counterexamples
</p>

<p>
Large-scale learners can behave unpredictably, producing
overparameterized models that achieve high accuracy yet exhibit
inconsistent behavior. To better understand reliable learning, we
revisit the proper online learning model with equivalence queries,
introduced by Angluin (1988), where the learner must always output a
valid hypothesis from the concept class and learns through
counterexamples.
</p>

<p>
We establish improved bounds and extend the framework to multi-class
bandit feedback. Specifically, we generalize to a broader family of
counterexample distributions, derive improved rates for infinite
Littlestone classes and consider also partial feedback where the
hypotheses are multiclass but the feedback is a non-labeled mistake.
Our results build on a new proof of Angluin and Dohrn’s analysis,
showing that random counterexamples yield faster convergence for
finite classes, and extend this insight to richer and more realistic
feedback models.
</p>
</div>
</div>
</div>
</div>
</body>
</html>
